name: Run Benchmarks

# Run this action when triggered manually through GitHub UI.
on:
  workflow_dispatch:
    inputs:
      timeout:
        required: true
        type: number
        default: 20
      config:
        required: true
        type: choice
        default: safety
        options:
        - sat
        - unsat
        - nonterm
        - cpx
      benchmarks:
        required: true
        type: choice
        default: chc23-lia-lin
        options:
        - chc22-lia-lin
        - chc23-lia-lin

permissions:
  # deployments permission to deploy GitHub pages website
  deployments: write
  # contents permission to update benchmark contents in gh-pages branch
  contents: write

jobs:

  chc-comp-benchmark:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        ci_index: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]
        ci_total: [20]

    steps:

      - name: Checkout LoAT Repository
        uses: actions/checkout@v4
        with:
          sparse-checkout: .github/actions
          path: actions

      - if: ${{ inputs.benchmarks == 'chc23-lia-lin' }}
        uses: ./actions/.github/actions/chc23-lia-lin

      - if: ${{ inputs.benchmarks == 'chc22-lia-lin' }}
        uses: ./actions/.github/actions/chc22-lia-lin

      - uses: dawidd6/action-download-artifact@v3
        with:
          workflow: build-loat.yml
          name: loat-static-${{ github.ref_name }}
          path: /usr/local/bin/

      - name: Run Benchmark
        run: |
          # function to run a single benchmark
          function benchmark() {
            filename=$1
            printf "running ${filename} ... "
            start=`date +%s`

            set +e
            if [ ${{ inputs.config }} = "sat" ]; then
              result=$(timeout ${{ inputs.timeout }} loat-static --mode safety --format horn --proof-level 0 --engine til --til::mode interleaved "${filename}")
            elif [ ${{ inputs.config }} = "unsat" ]; then
              result=$(timeout ${{ inputs.timeout }} loat-static --mode reachability --format horn --proof-level 0 --engine abmc "${filename}")
            elif [ ${{ inputs.config }} = "nonterm" ]; then
              result=$(timeout ${{ inputs.timeout }} loat-static --mode non_termination --format its --proof-level 0 --engine adcl "${filename}")
            elif [ ${{ inputs.config }} = "cpx" ]; then
              result=$(timeout ${{ inputs.timeout }} loat-static --mode complexity --format koat --proof-level 0 --engine adcl "${filename}")
            fi

            exit_status=$?
            set -e

            end=`date +%s`
            runtime=$((end-start))

            if [[ $exit_status -eq 0 ]]; then
              result=$(echo "$result" | head -n 1)
            elif [[ $exit_status -eq 124 ]]; then
              result="timeout"
            else
              result="error"
            fi

            printf "${result} after ${runtime}s\n"

            echo ${result} > ${filename}.res
          }

          # make 'benchmark' available for 'parallel'
          export -f benchmark

          # permissions are not preserved for artifacts:
          chmod +x /usr/local/bin/loat-static

          cd benchmarks

          # remove benchmarks that are executed by other workers
          i=0
          for filename in *.smt2; do
            if [[ $(( i % CI_TOTAL )) -ne $CI_INDEX ]]; then
              rm ${filename}
            fi
            i=$(( i+1 ))
          done

          if [[ 0 -eq $CI_INDEX ]]; then
             loat-static --version > loat.version
          fi

          # execute benchmarks in parallel
          parallel benchmark ::: *.smt2
        env:
          CI_TOTAL: ${{ matrix.ci_total }}
          CI_INDEX: ${{ matrix.ci_index }}

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@master
        with:
          name: ${{ inputs.benchmarks }}-${{ matrix.ci_index }}
          path: |
            benchmarks/*.res
            benchmarks/*.version

  finalize-benchmark-results:
    needs: [chc-comp-benchmark]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/download-artifact@v4
        with:
          pattern: ${{ inputs.benchmarks }}*
      - name: Merge Benchmark Results
        run: |
          declare -A results
          version=$(cat ${{ inputs.benchmarks }}-0/loat.version)
          results="${{ inputs.benchmarks }}-$version.json"
          echo -e "{\n" >> $results
          echo -e "\t\"branch\": \"${{ github.ref_name }}\"," >> $results
          echo -e "\t\"sha\": \"$version\"," >> $results
          echo -e "\t\"results\": [" >> $results
          for filename in ${{ inputs.benchmarks }}*/*.res; do
            benchmark=$(basename "$filename" .res)
            res=$(cat "$filename")
            echo -e "\t\t{\"name\": \"$benchmark\", \"result\": \"$res\"}," >> $results
          done
          echo -e "\t]" >> $results
          echo -e "}" >> $results
      - uses: actions/upload-artifact@master
        with:
          name: ${{ inputs.benchmarks }}
          path: ${{ inputs.benchmarks }}-*.json

  # publish-benchmark-results:
  #   needs: [finalize-benchmark-results]
  #   runs-on: ubuntu-latest
  #   steps:
  #     - uses: actions/checkout@v4
  #     - uses: actions/download-artifact@v4
  #     - run: |
  #         echo "[" >> output.json
  #         cat ${{ inputs.repo }}/${{ inputs.repo }}.json >> output.json
  #         # remove trailing comma
  #         sed -i '$ s/,$//g' output.json
  #         echo "]" >> output.json
  #     - name: Publish benchmark result
  #       uses: benchmark-action/github-action-benchmark@v1
  #       with:
  #         name: LoAT Benchmarks
  #         # What benchmark tool the output.txt came from
  #         tool: 'customBiggerIsBetter'
  #         # Where the output from the benchmark tool is stored
  #         output-file-path: output.json
  #         # Access token to deploy GitHub Pages branch
  #         github-token: ${{ secrets.GITHUB_TOKEN }}
  #         # Push and deploy GitHub pages branch automatically
  #         auto-push: true
  #         gh-pages-branch: web
  #         benchmark-data-dir-path: docs/dev/bench
